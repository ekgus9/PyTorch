{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13_RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKYqnJ1YIYzjoKuu6+L7d2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- Recurrent Neural Network\n",
        "\n",
        "  <-> Feed Forward Neural Network\n",
        "\n",
        "  RNN : 은닉층 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징 (메모리 셀, 은닉 상태)"
      ],
      "metadata": {
        "id": "hgx7ZHlYmXDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "timesteps = 10 # 문장 길이\n",
        "input_size = 4 # 입력 차원\n",
        "hidden_size = 8# 은닉 상태 크기\n",
        "\n",
        "input = np.random.random((timesteps,input_size))\n",
        "hidden_state_t = np.zeros((hidden_size,))\n",
        "\n",
        "Wx = np.random.random((hidden_size, input_size))  # (8, 4)\n",
        "Wh = np.random.random((hidden_size, hidden_size)) # (8, 8)\n",
        "b = np.random.random((hidden_size,)) # (8,)\n",
        "\n",
        "total_hidden_states = []\n",
        "\n",
        "for input_t in input:\n",
        "  output_t = np.tanh(np.dot(Wx,input_t) + np.dot(Wh,hidden_state_t) + b)\n",
        "\n",
        "  total_hidden_states.append(list(output_t))\n",
        "  hidden_state_t = output_t\n",
        "\n",
        "total_hidden_states = np.stack(total_hidden_states, axis = 0) \n",
        "total_hidden_states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyrnhWIU0dI9",
        "outputId": "b12554b5-4638-4332-c9b6-3b159d8aa785"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.93421325, 0.86751853, 0.95754409, 0.93604264, 0.92807223,\n",
              "        0.94645895, 0.92916313, 0.89407269],\n",
              "       [0.99993133, 0.99945068, 0.9999921 , 0.99999185, 0.99983875,\n",
              "        0.99995518, 0.99986941, 0.99994417],\n",
              "       [0.99995205, 0.99980364, 0.99999722, 0.9999973 , 0.99990442,\n",
              "        0.99999159, 0.99995529, 0.9999887 ],\n",
              "       [0.9998668 , 0.99966697, 0.99999268, 0.99999629, 0.99980635,\n",
              "        0.99998978, 0.99993715, 0.99997638],\n",
              "       [0.99965458, 0.99937804, 0.99998191, 0.99999442, 0.9996179 ,\n",
              "        0.99998574, 0.99990692, 0.99995487],\n",
              "       [0.99982292, 0.99959729, 0.99998948, 0.99999614, 0.99976962,\n",
              "        0.99998921, 0.99992914, 0.99996462],\n",
              "       [0.9999598 , 0.99988175, 0.99999743, 0.99999824, 0.99991881,\n",
              "        0.99999456, 0.9999649 , 0.99998637],\n",
              "       [0.99983899, 0.99966291, 0.99998986, 0.99999547, 0.99972469,\n",
              "        0.9999835 , 0.99991637, 0.99996566],\n",
              "       [0.99996651, 0.99987637, 0.9999978 , 0.99999777, 0.99991543,\n",
              "        0.99999153, 0.99995872, 0.99998888],\n",
              "       [0.99998734, 0.99992402, 0.99999925, 0.9999986 , 0.99996482,\n",
              "        0.99999557, 0.99997646, 0.99999566]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- nn.RNN()"
      ],
      "metadata": {
        "id": "KY-ier1RBOyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_size = 5\n",
        "hidden_size = 8\n",
        "input = torch.Tensor(1,10,5)\n",
        "cell = nn.RNN(input_size,hidden_size,batch_first = True)"
      ],
      "metadata": {
        "id": "1wyV8oX-BSVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bidirectional Recurrent Neural Network\n",
        "\n",
        "  앞 시점과 뒤 시점의 은닉 상태를 전달 받는다."
      ],
      "metadata": {
        "id": "vjs1VnLfBzOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True, bidirectional = True)"
      ],
      "metadata": {
        "id": "YntQel2kCWv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Long Short-Term Memory, LSTM\n",
        "\n",
        "  vanilla RNN은 비교적 짧은 시점에서만 효과를 보인다. : the problem of Long-Term Dependencies\n",
        "\n",
        "  LSTM은 은닉층 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 긴 시퀀스 입력에 유리하다."
      ],
      "metadata": {
        "id": "m3pCyxOCCpQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.LSTM(input_size, hidden_size, batch_fisrt=True)  "
      ],
      "metadata": {
        "id": "1kT0djf1C3Q4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}