{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "14_CharRNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO8+qDQI5+9X3rVgMGEUEGe"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- Char RNN\n",
        "\n",
        "  문자 단위 RNN"
      ],
      "metadata": {
        "id": "gMiFMkBkEiuL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXznZHoKEI4Z",
        "outputId": "d3512b0e-647d-49eb-a1cb-d29d53a8cbf3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "\n",
        "input_size = vocab_size\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1\n",
        "\n",
        "char_to_index = dict((c, i) for i,c in enumerate(char_vocab))\n",
        "\n",
        "index_to_char = {}\n",
        "for key,value in char_to_index.items():\n",
        "  index_to_char[value] = key \n",
        "\n",
        "index_to_char"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WINYeEekQvO",
        "outputId": "ece8e771-d8f0-477e-98a3-19009a8e9884"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Pytorch nn.RNN() 은 3차원 텐서를 입력 받기 때문에 배치 차원 추가\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "\n",
        "# 각 문자 one-hot vector 화\n",
        "\n",
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data] # np.eye(a) : n * n 의 단위 행렬 생성\n",
        "x_one_hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUVW1xPQl6g3",
        "outputId": "42f887bb-6920-465c-fc35-d0153fd84d1d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0., 0.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5l2Ubsfn6K7",
        "outputId": "97530684-e78b-4b22-9b66-7778211506ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(Net,self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first = True)\n",
        "    self.fc = torch.nn.Linear(hidden_size, output_size, bias = True)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "model = Net(input_size, hidden_size, output_size)\n",
        "\n",
        "outputs = model(X)\n",
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1neptGAapf_y",
        "outputId": "32ce8c5f-28bc-4e7c-b614-2aeea589523b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.view(-1,input_size).shape # batch  차원 제거"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj8zHSYps0-K",
        "outputId": "4c429238-458b-479e-d01d-29df97d4f430"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), learning_rate)\n",
        "\n",
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  outputs=model(X)\n",
        "  loss = criterion(outputs.view(-1, input_size), Y.view(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  result = outputs.data.numpy().argmax(axis=2)\n",
        "  result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "  print(i,'loss: ', loss.item(), 'prediction: ', result, 'true Y: ', y_data, 'prediction str: ',result_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvT4jl3WtINE",
        "outputId": "5a80e491-f458-4f7b-ab43-452fcf732b09"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss:  1.619940996170044 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "1 loss:  1.4305297136306763 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "2 loss:  1.1994540691375732 prediction:  [[4 4 4 2 3]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppel\n",
            "3 loss:  1.04063880443573 prediction:  [[4 4 3 0 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppl!!\n",
            "4 loss:  0.8797737956047058 prediction:  [[4 4 3 0 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppl!!\n",
            "5 loss:  0.7086673974990845 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "6 loss:  0.5679048299789429 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "7 loss:  0.4600202143192291 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "8 loss:  0.35548874735832214 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "9 loss:  0.2531050443649292 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.17386174201965332 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.12554192543029785 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.095330610871315 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.07372196018695831 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.05841885879635811 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.04781435802578926 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.04004896432161331 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.03382112458348274 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.028501052409410477 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.02386820688843727 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.019867248833179474 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.016475427895784378 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.013655307702720165 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.011348383501172066 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.009484382346272469 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.007990861311554909 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.006799805909395218 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.00585143081843853 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.00509528536349535 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.004490183666348457 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.004003037698566914 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.0036079897545278072 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.003284849226474762 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.0030180576723068953 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.002795681357383728 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.0026085807476192713 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.0024496836122125387 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.0023134988732635975 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.0021959757432341576 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.0020937002263963223 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.0020040874369442463 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.0019250247860327363 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.0018548739608377218 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.001792306313291192 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0017361333593726158 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.0016855003777891397 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.0016395511338487267 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.0015976683935150504 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.001559162512421608 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.0015237723710015416 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0014909743331372738 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.001460412167944014 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.001431752578355372 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.0014048529556021094 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0013795947888866067 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.001355620683170855 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.001333002233877778 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.0013114538742229342 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.0012908803764730692 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.001271377201192081 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.0012528013903647661 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.0012350337347015738 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0012180509511381388 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.0012018049601465464 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.0011863199761137366 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.001171524403616786 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.0011572993826121092 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.0011436686618253589 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.001130679971538484 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.0011180953588336706 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.0011061053955927491 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.001094543025828898 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.0010834087152034044 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.0010726071195676923 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.0010622574482113123 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.0010522643569856882 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.001042508869431913 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.0010330863296985626 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.0010239965049549937 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.0010150970192626119 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.0010064591187983751 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.0009980825707316399 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.0009898485150188208 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.0009818526450544596 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.0009740705718286335 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.0009664071840234101 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.00095891032833606 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.0009516036952845752 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0009444396710023284 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.0009373468346893787 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.0009304445120505989 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.0009236608748324215 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.0009169484255835414 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.0009103549527935684 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.0009038803400471807 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.0008975484524853528 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.0008912640623748302 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.0008850746671669185 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.000879051978699863 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.000873005308676511 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - word RNN - 임베딩 사용\n",
        " "
      ],
      "metadata": {
        "id": "YwBZTM4sxS1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "sentence = 'Repeat is the vest medicine for memory'.split()\n",
        "\n",
        "vocab = list(set(sentence))\n",
        "\n",
        "word2 = {tkn: i for i, tkn in enumerate(vocab,1)}\n",
        "word2['<unk>']=0\n",
        "\n",
        "index2 = {v: k for k, v in word2.items()}\n",
        "index2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmL8ImCTx1fH",
        "outputId": "694c6564-e892-4372-c21e-d5d2b6028b39"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '<unk>',\n",
              " 1: 'memory',\n",
              " 2: 'medicine',\n",
              " 3: 'vest',\n",
              " 4: 'the',\n",
              " 5: 'is',\n",
              " 6: 'for',\n",
              " 7: 'Repeat'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Text classification using PyTorch\n",
        "\n",
        "  RNN의 다-대-일 문제\n",
        "\n",
        "  IMDB Movie Review Sentiment Analysis"
      ],
      "metadata": {
        "id": "A_jQ1fEQz-zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.backends.cuda import cuBLASModule\n",
        "import os\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext.legacy import data,datasets\n",
        "import random\n",
        "\n",
        "batch_size = 64\n",
        "lr = 0.001\n",
        "epochs = 10\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "text = data.Field(sequential = True, batch_first = True, lower = True) \n",
        "label = data.Field(sequential = False, batch_first = True)\n",
        "\n",
        "trainset, testset = datasets.IMDB.splits(text, label)\n",
        "\n",
        "text.build_vocab(trainset,min_freq = 5)\n",
        "label.build_vocab(trainset)\n",
        "\n",
        "vocab_size = len(text.vocab)\n",
        "n_classes = 2\n",
        "\n",
        "\n",
        "trainset, valset = trainset.split(split_ratio=0.8)\n",
        "\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits((trainset, valset, testset), batch_size = batch_size, shuffle = True, repeat = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsLy_Mkt0ljU",
        "outputId": "ead3b734-9130-4f50-d46f-16883fc628a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84.1M/84.1M [00:02<00:00, 29.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "  def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p = 0.2):\n",
        "    super(GRU, self).__init__()\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.embed = nn.Embedding(n_vocab, embed_dim)\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "    self.gru = nn.GRU(embed_dim, self.hidden_dim, num_layers = self.n_layers, batch_first = True)\n",
        "    self.out = nn.Linear(self.hidden_dim, n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x =self.embed(x)\n",
        "    h = self._init_state(batch_size = x.size(0))\n",
        "    x, _ = self.gru(x,h)\n",
        "    h_t = x[:,-1,:]\n",
        "    self.dropout(h_t)\n",
        "    logit = self.out(h_t)\n",
        "    return logit\n",
        "\n",
        "  def _init_state(self,batch_size = 1):\n",
        "    weight = next(self.parameters()).data\n",
        "    return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"
      ],
      "metadata": {
        "id": "tOpK7KY36pMy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRU(1,256, vocab_size, 128, n_classes, 0.5).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = lr)"
      ],
      "metadata": {
        "id": "5gXAjQHATqgw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer,train_iter):\n",
        "  model.train()\n",
        "  for b, batch in enumerate(train_iter):\n",
        "    x,y = batch.text.to(device), batch.label.to(device)\n",
        "    y.data.sub_(1)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logit = model(x)\n",
        "    loss = F.cross_entropy(logit,y)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "BEmtcOZUUm8V"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_iter):\n",
        "    model.eval()\n",
        "    corrects, total_loss = 0, 0\n",
        "    for batch in val_iter:\n",
        "        x, y = batch.text.to(device), batch.label.to(device)\n",
        "        y.data.sub_(1) \n",
        "        logit = model(x)\n",
        "        loss = F.cross_entropy(logit, y, reduction='sum')\n",
        "        total_loss += loss.item()\n",
        "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
        "    size = len(val_iter.dataset)\n",
        "    avg_loss = total_loss / size\n",
        "    avg_accuracy = 100.0 * corrects / size\n",
        "    return avg_loss, avg_accuracy\n"
      ],
      "metadata": {
        "id": "ttB0t_2yWgaJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = None\n",
        "for e in range(1, epochs+1):\n",
        "    train(model, optimizer, train_iter)\n",
        "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
        "\n",
        "    print(\"[Epoch: %d] val loss : %5.2f | val accuracy : %5.2f\" % (e, val_loss, val_accuracy))\n",
        "\n",
        "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        if not os.path.isdir(\"snapshot\"):\n",
        "            os.makedirs(\"snapshot\")\n",
        "        torch.save(model.state_dict(), './snapshot/txtclassification.pt')\n",
        "        best_val_loss = val_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smO4AtcZXel8",
        "outputId": "8b7642c5-ec17-47f1-b26a-ba723efc0e97"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 1] val loss :  0.70 | val accuracy : 49.52\n",
            "[Epoch: 2] val loss :  0.69 | val accuracy : 50.28\n",
            "[Epoch: 3] val loss :  0.69 | val accuracy : 51.02\n",
            "[Epoch: 4] val loss :  0.69 | val accuracy : 52.94\n",
            "[Epoch: 5] val loss :  0.69 | val accuracy : 52.08\n",
            "[Epoch: 6] val loss :  0.63 | val accuracy : 68.64\n",
            "[Epoch: 7] val loss :  0.41 | val accuracy : 82.76\n",
            "[Epoch: 8] val loss :  0.35 | val accuracy : 85.78\n",
            "[Epoch: 9] val loss :  0.35 | val accuracy : 85.70\n",
            "[Epoch: 10] val loss :  0.35 | val accuracy : 86.40\n"
          ]
        }
      ]
    }
  ]
}